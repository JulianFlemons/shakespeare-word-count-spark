{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 23:22:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ShakespeareHW\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a text file (example)\n",
    "rdd = sc.textFile(\"shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/julianflemons8/hw0_Shakespeare_txt_spark/shakespeare.txt\"  \n",
    "shakespeare_text_rdd = sc.textFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen:', 'before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split each line into words and convert to lowercase\n",
    "words_lowercase_rdd = rdd.flatMap(lambda line: [word.lower() for word in line.split()])\n",
    "\n",
    "# Collect the results\n",
    "words_lowercase = words_lowercase_rdd.collect()\n",
    "\n",
    "# Print the results\n",
    "print(words_lowercase[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Remove punctuation\n",
    "cleaned_words_rdd = words_lowercase_rdd.map(lambda word: re.sub(r'[^\\w\\s]', '', word))\n",
    "cleaned_words_rdd = cleaned_words_rdd.filter(lambda word: word != '') # Remove any empty strings that might result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = cleaned_words_rdd.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count word occurrences \n",
    "word_count = words_rdd.countByValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count word occurrences\n",
    "word_count = cleaned_words_rdd.countByValue()\n",
    "\n",
    "# Convert the resulting dictionary to an RDD of (word, count) pairs\n",
    "word_counts_rdd = sc.parallelize(list(word_count.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequent words (without preprocessing):\n",
      "the: 6283\n",
      "and: 5680\n",
      "to: 4766\n",
      "i: 4653\n",
      "of: 3757\n",
      "you: 3142\n",
      "my: 3118\n",
      "a: 2987\n",
      "that: 2569\n",
      "in: 2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort by count in descending order\n",
    "sorted_word_counts_rdd = word_counts_rdd.sortBy(lambda item: item[1], ascending=False)\n",
    "\n",
    "# Take the top 10 words\n",
    "top_10_words_no_preprocessing = sorted_word_counts_rdd.take(10)\n",
    "\n",
    "# Print the results for Part 1\n",
    "print(\"Top 10 frequent words (without preprocessing):\")\n",
    "for word, count in top_10_words_no_preprocessing:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 frequent words (with stop word filtering):\n",
      "thou: 1403\n",
      "thy: 1059\n",
      "king: 887\n",
      "shall: 845\n",
      "thee: 760\n",
      "lord: 698\n",
      "good: 669\n",
      "come: 624\n",
      "sir: 592\n",
      "well: 563\n"
     ]
    }
   ],
   "source": [
    "# --- Part 3: Top 10 frequent words with stop word filtering ---\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_words_rdd = cleaned_words_rdd.filter(lambda word: word not in stop_words)\n",
    "\n",
    "# Count word occurrences after filtering stop words\n",
    "word_counts_filtered = filtered_words_rdd.countByValue()\n",
    "\n",
    "# Convert to RDD of (word, count) pairs\n",
    "word_counts_filtered_rdd = sc.parallelize(list(word_counts_filtered.items()))\n",
    "\n",
    "# Sort by count in descending order\n",
    "sorted_word_counts_filtered_rdd = word_counts_filtered_rdd.sortBy(lambda item: item[1], ascending=False)\n",
    "\n",
    "# Take the top 10 words after filtering\n",
    "top_10_words_with_stopwords = sorted_word_counts_filtered_rdd.take(10)\n",
    "\n",
    "# Print the results for Part 3\n",
    "print(\"\\nTop 10 frequent words (with stop word filtering):\")\n",
    "for word, count in top_10_words_with_stopwords:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop Spark Session (moved here to be at the end of the script)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
